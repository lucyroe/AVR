{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AffectiveVR","text":"<p><code>[Last update: September 16, 2024]</code></p> <pre><code>Period:     2023-10 - 2024-09\nStatus:     completed\n\nAuthor(s):  Lucy Roellecke\nContact:    lucy.roellecke[at]tuta.com\n</code></pre>"},{"location":"#links","title":"Links","text":""},{"location":"#github-repository","title":"GitHub Repository","text":"<p>The GitHub Repository with all the code can be assessed here: github.com/lucyroe/AVR</p>"},{"location":"#code-documentation","title":"Code Documentation","text":"<p>The Code Documentation can be assessed here: lucyroe.github.io/AVR</p>"},{"location":"#project-description","title":"Project description","text":"<p>Affective VR (AVR) aims to develop and test a tool for continous emotion ratings. The project proposes such a tool and assesses its effectiveness, usability and reliability using videos presented in virtual reality (VR).</p>"},{"location":"#project-structure","title":"Project structure","text":"<p>The AVR project consists of three main stages: In the Selection Phase, three different rating methods ('Grid', 'Flubber' and 'Proprioceptive') were tested with different videos in VR of each 1 min length. In the Evaluation Phase, the 'Flubber' as winning rating method from the Selection Phase was tested for a longer VR experience of about 20 min with different videos playing after one another. In the Physio Phase, the very same stimuli and rating method are used but additionally to the behavioral data, EEG and periphysiological data are acquired.  </p> <p></p> <p>This repository mainly focuses on the Physio Phase as it contains code I wrote in the framework of my master thesis with the title Embodied Emotion: Decoding dynamic affective states by integrating neural and cardiac data. However, this repository also contains code and results of earlier phase of the project by PhD Candidate Antonin Fourcade that I could use and profit off during the analyses for the thesis.    </p> <p>Code for all three phases can be found in <code>./code/AVR</code>.  The whole preprocessing and analysis pipeline can be followed by running <code>main.py</code>.  The directory <code>./code/AVR/preprocessing</code> contains scripts to preprocess annotation data and physiological data, respectively, of AVR Physio Phase.  The directory <code>./code/AVR/statistics</code> contains scripts to calculate both univariate statistics, as well as modelling statistics.   The directory <code>./code/AVR/datacomparison</code> contains scripts that compare continuous ratings from the Selection phase of the AVR project with the Physio Phase of the AVR project.   The directory <code>./code/AVR/modelling</code> contains scripts to perform a Hidden Markov Model (HMM) analysis on data from the Physio phase to identify hidden affective states by using the physiological data.   The directory <code>./code/AVR/datavisualization</code> contains plotting functions.   </p> <pre><code> \ud83d\udcc2 code\n \u251c\u2500\u2500 \ud83d\udc0d main.py\n \u251c\u2500\u2500 \ud83d\udcc2 AVR\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 datacomparison\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 datavisualization\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 statistics\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 modelling\n \u2502   \u2514\u2500\u2500 \ud83d\udcc1 preprocessing\n \u2502       \u251c\u2500\u2500 \ud83d\udcc1 annotation\n \u2502       \u2514\u2500\u2500 \ud83d\udcc1 physiological \n \u2514\u2500\u2500 \ud83d\udcc1 configs\n</code></pre> <p>The results of all analyses can be found in <code>./results</code>.   There is one sub-directory in the main results directory for phase of the AVR project: <code>./results/phase1</code>, <code>./results/phase2</code>, <code>./results/phase3</code>, and <code>./results/comparison_phase1_phase2</code> and <code>./results/comparison_phase1_phase3</code> for comparing Selection and Evaluation/Physio phase.    </p> <pre><code> \ud83d\udcc2 results\n \u251c\u2500\u2500 \ud83d\udcc1 comparison_phase1_phase2\n \u251c\u2500\u2500 \ud83d\udcc1 comparison_phase1_phase3\n \u251c\u2500\u2500 \ud83d\udcc1 phase1\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 assessment_results\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 cocor_results\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 cor_results\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 cr_plots\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 datacomparison\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 datavisualization\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 descriptives\n \u2502   \u2514\u2500\u2500 \ud83d\udcc1 icc_results\n \u251c\u2500\u2500 \ud83d\udcc1 phase2\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 cpa\n \u2502   \u2514\u2500\u2500 \ud83d\udcc1 descriptives\n \u2514\u2500\u2500 \ud83d\udcc1 phase3\n      \u2514\u2500\u2500 \ud83d\udcc1 AVR\n</code></pre> <p>The figures published in my master thesis and the manuscripts can be found in <code>./publications</code>.    </p> <pre><code> \ud83d\udcc2 publications\n \u251c\u2500\u2500 \ud83d\udcc1 articles\n \u2502   \u2514\u2500\u2500 \ud83d\udcc1 figures\n \u2514\u2500\u2500 \ud83d\udcc1 thesis\n      \u2514\u2500\u2500 \ud83d\udcc1 figures\n</code></pre>"},{"location":"#install-research-code-as-package","title":"Install research code as package","text":"<p>In case there is no project-related virtual / conda environment yet, create one for the project:</p> <pre><code>conda create -n AVR_3.11 python=3.11\n</code></pre> <p>And activate it:</p> <pre><code>conda activate AVR_3.11\n</code></pre> <p>Then install the code of the research project as python package:</p> <pre><code># assuming your current working dircetory is the project root\npip install -e \".[develop]\"\n</code></pre> <p>Note: The <code>-e</code> flag installs the package in editable mode, i.e., changes to the code will be directly reflected in the installed package. Moreover, the code keeps its access to the research data in the underlying folder structure. Thus, the <code>-e</code> flag is recommended to use.</p>"},{"location":"#contributorscollaborators","title":"Contributors/Collaborators","text":"<p>Antonin Fourcade  Francesca Malandrone  Michael Gaebler   </p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"code/","title":"AffectiveVR \u2013 code","text":"<pre><code>Last update:    September 16, 2024\nStatus:         completed\n</code></pre>"},{"location":"code/#description","title":"Description","text":"<p>The whole preprocessing and analysis pipeline can be followed by running <code>main.py</code>.  </p> <p>The main analysis method used in this project is Hidden Markov Models (HMMs). A Hidden Markov Model (HMM) is a data-driven generative statistical model that can be used to identify hidden states in dynamically changing systems, e.g., the brain or the heart (Khalifa et al., 2021; Quinn et al., 2018). The hidden states are mutually exclusive, such that only one state occurs at any one point in time, and Markovian, such that one state is only dependent on the previous state. Each state has an initial probability \ud835\udf45. As the states are hidden and cannot be observed directly in the data, observation models (or emission probabilities, \ud835\udf3d) link the data with each hidden state, representing the probability distribution from which the data is drawn while the state is active. Transitions between the states are represented in the transition probability matrix A. Given only a set of observations X, the Baum-Welch algorithm can be used to iteratively compute a set of model parameters \u03bb = (\ud835\udf45, A, \ud835\udf3d) using both forward and Viterbi algorithms based on the expectation-maximization method. The such identified states can then be compared across different models or conditions.</p> <p>HMMs were implemented in this project using the Python package <code>hmmlearn</code> (version 0.3.2).</p> <p></p>"},{"location":"code/#preprocessing","title":"Preprocessing","text":"<p>The script <code>read_xdf.py</code> reads in all data from the LSL-output format <code>.xdf</code>, allows for manual checks, and converts it into BIDS-compatible files and folder structure.  Two different directories exist for the preprocessing of annotation (<code>./code/AVR/preprocessing/annotation</code>) and physiological data (<code>./code/AVR/preprocessing/physiological</code>).</p> <pre><code> \ud83d\udcc2 preprocessing\n \u251c\u2500\u2500 \ud83d\udcc1 annotation\n \u2502   \u251c\u2500\u2500 \ud83d\udc0d preprocessing_annotation_avr_phase1.py\n \u2502   \u251c\u2500\u2500 \ud83d\udc0d preprocessing_annotation_avr_phase2.py\n \u2502   \u2514\u2500\u2500 \ud83d\udc0d preprocessing_annotation_avr_phase3.py\n \u2514\u2500\u2500 \ud83d\udcc1 physiological\n     \u251c\u2500\u2500 \ud83d\udc0d preprocessing_physiological_avr_phase3.py\n     \u251c\u2500\u2500 \ud83d\udc0d preprocessing_metadata.py\n     \u251c\u2500\u2500 \ud83d\udc0d bad_after_cleaning_check.py\n     \u2514\u2500\u2500 \ud83d\udc0d feature_extraction.py\n</code></pre>"},{"location":"code/#annotation","title":"Annotation","text":"<p>Preprocessing of annotation data consists of these steps (for phase 3; for phase 1+2, see respective scripts):</p> <ol> <li>Cropping 2.5 seconds at the start and end of the data to avoid edge artifacts.   </li> <li>Downsampling to 1 Hz.   </li> <li>Interpolating missing values linearly so the data of all participants has the same shape.   </li> </ol>"},{"location":"code/#physiological","title":"Physiological","text":""},{"location":"code/#ecg","title":"ECG","text":"<p>Preprocessing of ECG data consists of these steps:  </p> <ol> <li>Cropping 2.5 seconds at the start and end of the data to avoid edge artifacts.  </li> <li>Cleaning: 50 Hz power line noise removal, Band-pass filtering between 0.5 and 30 Hz.</li> <li>R-Peak Detection: Automated R-Peak Detection using <code>neurokit2</code> (Open Source Python Toolbox), manual check of detected R-peaks in interactive window.</li> <li>IBI Calculation</li> </ol> <p>Feature extraction of ECG data consists of these steps:  </p> <ol> <li>Resampling to 4 Hz.</li> <li>Symmetric Padding of 90 seconds mirrored data to the start and end of the data.</li> <li>Continuous Wavelet Transform (CWT) using <code>fCWT</code> (Python Library) to get the Time Frequency Representation (TFR).</li> <li>Averaging over 2s windows with 50% overlap.</li> <li>Integrating over frequency bands to get low-frequency (LF-) and high-frequency (HF-) heart rate variability (HRV).</li> </ol>"},{"location":"code/#eeg","title":"EEG","text":"<p>Preprocessing of EEG data consists of these steps: </p> <ol> <li>Cropping 2.5 seconds at the start and end of the data to avoid edge artifacts.  </li> <li>Cleaning: 50 Hz power line noise removal, re-referencing to average, band-pass filtering between 0.1 and 45 Hz, segmenting into epochs of 10s length, rejection or interpolation of bad channels and epochs using <code>autoreject</code> on 1 Hz-filtered data.</li> <li>Independent Component Analysis (ICA) to identify eye, cardiac, and muscle artifacts.</li> <li>Final threshold check to exclude any participants with more than 30% of remaining noisy epochs.</li> </ol> <p>Feature extraction of EEG data consists of these steps:  </p> <ol> <li>Resampling to 100 Hz.</li> <li>Symmetric Padding of 90 seconds mirrored data to the start and end of the data.</li> <li>Continuous Wavelet Transform (CWT) using <code>fCWT</code> (Python Library) to get the Time Frequency Representation (TFR).</li> <li>Averaging over 2s windows with 50% overlap.</li> <li>Integrating over frequency bands to get alpha, beta, gamma, delta and theta power values.</li> <li>Averaging across regions of interest (ROIs): posterior, frontal, whole-brain.</li> </ol> <p></p>"},{"location":"code/#modelling","title":"Modelling","text":"<p>In (<code>./code/AVR/modelling</code>) you can find one script that performs a Hidden Markov Model (HMM) analysis on the data from the Physio phase of the AVR project (<code>hmm.py</code>) and one script that compares the five models trained in the previous script in terms of their performance in decoding hidden affective states (<code>compare_models.py</code>).</p> <pre><code> \ud83d\udcc2 modelling\n \u251c\u2500\u2500 \ud83d\udc0d compare_models.py\n \u2514\u2500\u2500 \ud83d\udc0d hmm.py\n</code></pre> <p>Read the first paragraph of this README to learn about HMM. The five models trained in this thesis are:</p> <ul> <li>A cardiac model, trained only on IBI &amp; HF-HRV.</li> <li>A neural model, trained on posterior alpha, frontal alpha, frontal theta, whole-brain gamma &amp; whole-brain beta.</li> <li>An integrated model, trained on all seven of these features.</li> <li>A multimodal model, trained on the seven physiological features and the rating data.</li> <li>A subjective model, trained only on the rating data.</li> </ul> <p>The models are then compared in terms of...</p> <ul> <li>The general model quality (log-likelihood, AIC, BIC).</li> <li>The model's accuracy (correlation with subjective model &amp; fraction of corresponding states).</li> <li>The distance between the model's states and the states identified by the subjective model.</li> </ul>"},{"location":"code/#statistics","title":"Statistics","text":"<p>In (<code>./code/AVR/statistics</code>) you can find one script that calculates univariate statistics for differences in features between different videos (<code>univariate_statistics.py</code>), one script that calculates statistics for differences in features between different hidden states (<code>hmm_stats.py</code>) and one script that fits a general linear model (GLM) to the hidden states as identified in each HMM to test the states for significance (<code>glm.py</code>).</p> <pre><code> \ud83d\udcc2 statistics\n \u251c\u2500\u2500 \ud83d\udc0d univariate_statistics.py\n \u251c\u2500\u2500 \ud83d\udc0d glm.py\n \u2514\u2500\u2500 \ud83d\udc0d hmm_stats.py\n</code></pre>"},{"location":"code/#data-comparison","title":"Data Comparison","text":"<p>In (<code>./code/AVR/datacomparison</code>) you can find scripts to compare the variability in ratings between the Selection Phase and the Evaluation/Physio Phase of the project AVR, respectively.</p> <pre><code> \ud83d\udcc2 datacomparison\n \u251c\u2500\u2500 \ud83d\udc0d compare_variability_phase1_phase2.py\n \u2514\u2500\u2500 \ud83d\udc0d compare_variability_phase1_phase3.py\n</code></pre>"},{"location":"code/#data-visualization","title":"Data Visualization","text":"<p>In (<code>./code/AVR/datavisualization</code>) you can find scripts for visualizing the results. <code>radar_plot.py</code>creates a radar plot to visualize AVR questionnaire data from the Selection phase. <code>raincloud_plot.py</code> creates raincloud plots to visualize the difference in variability between the Selection phase and the Evaluation/Physio phase. <code>plot_descriptives.py</code>plots the mean ratings, cardiac and neural features of the Physio phase across time. And <code>plot_hidden_states.py</code>creates plots of the differences of the features between the hidden states as identified by each of the four HMMs.</p> <pre><code> \ud83d\udcc2 datavisualization\n \u251c\u2500\u2500 \ud83d\udc0d plot_descriptives.py\n \u251c\u2500\u2500 \ud83d\udc0d plot_hidden_states.py\n \u251c\u2500\u2500 \ud83d\udc0d radar_plot.py\n \u2514\u2500\u2500 \ud83d\udc0d raincloud_plot.py\n</code></pre>"},{"location":"code/#codebase","title":"Codebase","text":"<p>All code is written in Python 3.11. See below on how to install the code as a python package.</p>"},{"location":"code/#python","title":"Python","text":"<p>Python code (in the structure of a python package) is stored in <code>./code/AVR/</code></p> <p>To install the research code as package, run the following code in the project root directory:</p> <pre><code>pip install -e \".[develop]\"\n</code></pre>"},{"location":"code/#configs","title":"Configs","text":"<p>Paths to data, parameter settings, etc. are stored in the config file: <code>./code/configs/config.toml</code></p> <p>Private config files that contain, e.g., passwords, and therefore should not be shared, or mirrored to a remote repository can be listed in: <code>./code/configs/private_config.toml</code></p> <p>Both files will be read out by the script in <code>./code/AVR/configs.py</code>. Keep both config toml files and the script in the places, where they are.</p> <p>To use your configs in your python scripts, do the following:</p> <pre><code>from AVR.configs import config, paths\n\n# check out which paths are set in config.toml\npaths.show()\n\n# get the path to data\npath_to_data = paths.DATA\n\n# Get parameter from config\nweight_decay = config.params.weight_decay\n\n# Get private parameter from config\napi_key = config.service_x.api_key\n</code></pre> <p>Fill the corresponding <code>*config.toml</code> files with your data.</p> <p>For other programming languages, corresponding scripts must be implemented to use these <code>*config.toml</code> files in a similar way.</p>"},{"location":"code/#license","title":"LICENSE","text":"<p>MIT</p>"},{"location":"publications/","title":"AffectiveVR \u2013 publications","text":"<pre><code>Last update:    September 16, 2024\nStatus:         in preparation\n</code></pre>"},{"location":"publications/#description-of-publications","title":"Description of publications","text":"<p>This directory contains the figures in the publications. In (<code>./publications/articles/figures</code>) you can find figures for the Selection and Evaluation phase of the AVR project that will be published in articles (currently in preparation). In (<code>./publications/thesis/figures</code>) you can find figures for my master thesis.</p> <pre><code> \ud83d\udcc2 publications\n \u251c\u2500\u2500 \ud83d\udcc1 articles\n \u2502   \u2514\u2500\u2500 \ud83d\udcc1 figures\n \u2502       \u251c\u2500\u2500 \ud83d\udcca phase1_experimental_setup.png\n \u2502       \u251c\u2500\u2500 \ud83d\udcca phase1_experimental_setup.svg\n \u2502       \u251c\u2500\u2500 \ud83d\udcca phase1_rating_methods.png\n \u2502       \u251c\u2500\u2500 \ud83d\udcca phase1_rating_methods.svg\n \u2502       \u251c\u2500\u2500 \ud83d\udcca phase2_experimental_setup.png\n \u2502       \u2514\u2500\u2500 \ud83d\udcca phase2_experimental_setup.svg\n \u2514\u2500\u2500 \ud83d\udcc1 thesis\n     \u2514\u2500\u2500 \ud83d\udcc1 figures\n         \u251c\u2500\u2500 \ud83d\udcca AVR_Title.png\n         \u251c\u2500\u2500 \ud83d\udcca phase3_experimental_setup.png\n         \u251c\u2500\u2500 \ud83d\udcca phase3_experimental_setup.svg\n         \u251c\u2500\u2500 \ud83d\udcca phase3_preprocessing.png\n         \u251c\u2500\u2500 \ud83d\udcca phase3_preprocessing.svg\n         \u251c\u2500\u2500 \ud83d\udcca phase3_hmm.png\n         \u251c\u2500\u2500 \ud83d\udcca phase3_hmm.svg\n         \u251c\u2500\u2500 \ud83d\udcca phase3_cardiac_model.png\n         \u251c\u2500\u2500 \ud83d\udcca phase3_cardiac_model.svg\n         \u251c\u2500\u2500 \ud83d\udcca phase3_neural_model.png\n         \u251c\u2500\u2500 \ud83d\udcca phase3_neural_model.svg\n         \u251c\u2500\u2500 \ud83d\udcca phase3_integrated_model.png\n         \u251c\u2500\u2500 \ud83d\udcca phase3_integrated_model.svg\n         \u251c\u2500\u2500 \ud83d\udcca phase3_multimodal_model.png\n         \u251c\u2500\u2500 \ud83d\udcca phase3_multimodal_model.svg\n         \u251c\u2500\u2500 \ud83d\udcca phase3_subjective_model.png\n         \u2514\u2500\u2500 \ud83d\udcca phase3_subjective_model.svg\n</code></pre>"},{"location":"publications/#license","title":"License","text":"<p>MIT</p>"},{"location":"results/","title":"AffectiveVR \u2013 results","text":"<pre><code>Last update:    September 16, 2024\nStatus:         completed\n</code></pre>"},{"location":"results/#description-of-results","title":"Description of results","text":"<p>This directory contains the results of the Selection, the Evaluation, and the Physio phase of the AVR project. However, most of the results from the Selection and Evaluation phase have not been found by me but by Antonin Fourcade. As my thesis focuses on the Physio phase, results from that phase are most prominent here and can be found in <code>./results/phase3/AVR</code>. There, results for all subjects are listed separately, as well as averaged across all subjects. Some of the result plots are shown below.</p> <pre><code> \ud83d\udcc2 results\n \u251c\u2500\u2500 \ud83d\udcc1 comparison_phase1_phase2\n \u251c\u2500\u2500 \ud83d\udcc1 comparison_phase1_phase3\n \u251c\u2500\u2500 \ud83d\udcc1 phase1\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 assessment_results\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 cocor_results\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 cor_results\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 cr_plots\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 datacomparison\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 datavisualization\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 descriptives\n \u2502   \u2514\u2500\u2500 \ud83d\udcc1 icc_results\n \u251c\u2500\u2500 \ud83d\udcc1 phase2\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 cpa\n \u2502   \u2514\u2500\u2500 \ud83d\udcc1 descriptives\n \u2514\u2500\u2500 \ud83d\udcc1 phase3\n      \u2514\u2500\u2500 \ud83d\udcc1 AVR\n</code></pre>"},{"location":"results/#univariate-results","title":"Univariate Results","text":""},{"location":"results/#comparison-of-variability-between-selection-and-physio-phase","title":"Comparison of Variability between Selection and Physio Phase","text":""},{"location":"results/#hmm-results","title":"HMM Results","text":""},{"location":"results/#cardiac-model","title":"Cardiac Model","text":""},{"location":"results/#neural-model","title":"Neural Model","text":""},{"location":"results/#integrated-model","title":"Integrated Model","text":""},{"location":"results/#multimodal-model","title":"Multimodal Model","text":""},{"location":"results/#subjective-model","title":"Subjective Model","text":""},{"location":"results/#license","title":"License","text":"<p>MIT</p>"}]}